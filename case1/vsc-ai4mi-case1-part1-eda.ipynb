{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 4 Materials Industry\n",
    "# Case study 1: Faulty steel plates\n",
    "# Notebook 1: Exploratory Data Analysis for tabular data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The dataset\n",
    "\n",
    "The dataset consists of a series of features describing 6 well-defined classes of defects and one class containing all other faults. The dataset was made available by the [Semeion research center](http://www.semeion.it/wordpress/)\n",
    "\n",
    "The following urls provide a link to the dataset itself and some example code:\n",
    "* [Dataset at UCI ML](http://archive.ics.uci.edu/ml/datasets/steel+plates+faults \"Faulty steel plate dataset\")\n",
    "* [Kaggle page with example code](https://www.kaggle.com/uciml/faulty-steel-plates \"Kaggle\")\n",
    "\n",
    "Type of dependent variables, which we will try to predict (7 Types of Steel Plates Faults):\n",
    "1. Pastry\n",
    "2. Z_Scratch\n",
    "3. K_Scatch\n",
    "4. Stains\n",
    "5. Dirtiness\n",
    "6. Bumps\n",
    "7. Other_Faults\n",
    "\n",
    "\n",
    "27 independent variables, which we will use to make predictions:\n",
    "#### Position of the defect:\n",
    "* X_Minimum: Positions of defects\n",
    "* X_Maximum: ''\n",
    "* Y_Minimum: ''\n",
    "* Y_Maximum: '' \n",
    "* Edges_Index: mean(xmax,xmin)/length of conveyor -- \"How far away are we from the edge of the conveyer\"\n",
    "\n",
    "\n",
    "#### Size of the defect:\n",
    "* X_Perimeter: Bigger than xmax - xmin so likely a non-straight connect - see correlation with differences\n",
    "* Y_Perimeter: Ditto total area / 1D size?\n",
    "* Outside_X_Index: (X_Maximum - X_Minimum)/Length_of_Conveyer  (Relative size of fault compared to total conveyer)\n",
    "* Edges_X_Index: (X_Maximum - X_Minimum) / X_Perimeter\n",
    "* Edges_Y_Index: (Y_Maximum - Y_Minimum) / Y_Perimeter\n",
    "* Log_X_Index: log of Outside_X_Index\n",
    "* Log_Y_Index: log of (Y_Maximum - Y_Minimum), scaled with a factor of ~2.3\n",
    "* Pixels_Areas: Area of the defect in pixels\n",
    "* LogOfAreas: log of pixel_areas\n",
    "* SigmoidOfAreas: sigmoid of pixel_areas\n",
    "\n",
    "\n",
    "#### Orientation of the defect:\n",
    "* Square_Index: Absolute value of the orientation index\n",
    "* Orientation_Index:  +- Sigmoid of squareness\n",
    "* Outside_Global_Index: Classification of orientation index: 0.5 is square, and 0 or 1 is x or y directed -- sgn(Orientation_Index)/2 + 0.5\n",
    "\n",
    "\n",
    "#### Properties of the production process:\n",
    "* Length_of_Conveyer: Total size of plate (in pixels) in the X direction.\n",
    "* TypeOfSteel_A300: A300 steel - https://www.zamet.it/en/Austenitic-Steel-A300/p/52\n",
    "* TypeOfSteel_A400: A400 steel - https://www.zamet.it/en/A400-series-Ferritic-Stainless-Steels/p/54\n",
    "* Steel_Plate_Thickness\n",
    "\n",
    "\n",
    "#### Luminosity/brightness of the defect:\n",
    "* Sum_of_Luminosity: Luminosity summed over the pixel areas\n",
    "* Minimum_of_Luminosity: Least-bright pixel value \n",
    "* Maximum_of_Luminosity: Brightest pixel value\n",
    "* Empty_Index: +- log X_Size\\*Y_Size/Pixel_Area\n",
    "* Luminosity_Index: Sum of luminosity/pixel areas -- Average luminosity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "The first step when doing machine learning is always to explore the data you're going to work with.\n",
    "\n",
    "* What is the type of data?\n",
    "* What does it represent?\n",
    "* What are the limitations?\n",
    "* Is the dataset balanced? Is each class roughly equally represented?\n",
    "* Which features correlate with each other? Does this make sense?\n",
    "* Which features correlate with which output class?\n",
    "\n",
    "Knowing what the data can and cannot tell you is crucial to know what kind of models can be built on top of it, and what these models can be capable of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary: Setting up the environment\n",
    "\n",
    "Basic functions like plotting and doing math. It's not important if not everything in this step makes sense to you right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is simply to install the packages we'll be using, this only needs to happen once.\n",
    "# It might be necessary to restart the kernel after this step\n",
    "\n",
    "# ! pip install seaborn\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install shap\n",
    "# ! pip install pyarrow\n",
    "# ! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # Math functions\n",
    "import pandas as pd # Pandas is used for handling databases, and will be used for reading and manipulating the data\n",
    "import matplotlib.pyplot as plt # Plot functions\n",
    "import seaborn as sns # More plot functions\n",
    "\n",
    "sns.set_palette('colorblind') # Making the plots colorblind-friendly\n",
    "sns.set_style('darkgrid') # More info at https://seaborn.pydata.org/tutorial/aesthetics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "First, we read the data from the file `faults.csv`. For this we use pandas (`pd`) and read it as a pandas dataframe. In this case all the columns we want to read have been explicitly named, this isn't always necessary, but it helps to know exactly what is being read and what the dataframe contains.\n",
    "\n",
    "After the dataframe has been read, we use the `describe()` function to get a summary of what is contained and what sorts of values we can find in each column.\n",
    "\n",
    "It is very important to check whether there isn't any identifying column present. For example, if you want to teach a model to rank materials based on a certain property, and you start from an ordered dataset that has the rank encoded as a feature, then the model will probably simply learn to identify the rank and ignore all the \"real\" information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('faults.csv',header=0,names=['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas',\n",
    "       'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity',\n",
    "       'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer',\n",
    "       'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness',\n",
    "       'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index',\n",
    "       'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas',\n",
    "       'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index',\n",
    "       'SigmoidOfAreas', 'Pastry', 'Z_Scratch', 'K_Scratch', 'Stains',\n",
    "       'Dirtiness', 'Bumps', 'Other_Faults'])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense for what the actual data looks like, we can use the `head()` function, which shows us the first 5 rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with tabular data, data stored in a table with columns and rows.\n",
    "\n",
    "A row is a specific data point and can be addressed with `.iloc` in much the same way as a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[20:25].T # The .T operator transposes a grid, exchanging rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column is what we call a __feature__. These are typically labeled with a name and can be called as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"X_Maximum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select multiple columns at once by passing a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"X_Maximum\", \"X_Minimum\"]] # Note the brackets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform arithmetic on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"X_Maximum\"] - df[\"X_Minimum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even store this as a new feature. This statement creates a new feature column `X_Size` and stores the result of the operation inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"X_Size\"] = df[\"X_Maximum\"] - df[\"X_Minimum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it's also useful to consider what information is being encoded in these features. We are looking at faults on steel plates. The `X_minimum`, `X_maximum`, `Y_minimum` and `Y_maximum` tell us the boundaries of where the fault starts and ends. But it might be more useful to store the location and size, instead.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Create three new features called `X_Center`, `Y_Center` and `Y_Size` which store the coordinates of the center of the defect and its size. Be aware that these feature names are case sensitive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `pandas.Dataframe.drop` command and remove the four original columns `X_minimum`, `X_maximum`, `Y_minimum` and `Y_maximum` from the dataframe, as we now have a more intuitive way of storing the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"X_Minimum\", \"X_Maximum\", \"Y_Minimum\", \"Y_Maximum\"])\n",
    "# We could keep the original features, but if we want to be able to interpret our models,\n",
    "# it helps to not have too many features that encode the same information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the dataset for use in our other notebook. `pandas.DataFrame.to_csv` writes it to a standard CSV file, readable by Excel and other spreadsheet software. When reading the CSV again pandas will have to interpret how to store the data. The `pandas.Dataframe.to_Feather` method stores data in a binary format which does not require parsing and is thus much faster to read. If you want to add more features don't forget to do this before saving the file, then later you can read it again in the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(\"vsc-ai4mi-case1-eda.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "Plots can be more easily interpreted than bare numbers, so it can be useful to plot an overview of certain things. For example, using a pie chart to see the distribution of classes, a histogram to see a distribution of values, or a scatterplot to see how two features might relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['X_Center'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what a plot looks like between the orientation index and square index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='Orientation_Index',y='Square_Index', color=\"darkblue\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it's clear that the Orientation Index encodes information about\n",
    "how elongated the defect is, and whether its long side is in the X- or Y-direction.\n",
    "The square index encodes whether the defect is strongly elongated (-1 or +1) or completely square (0).\n",
    "\n",
    "Note that a linear regression between these features would show a correlation of 0.\n",
    "This would imply they're not related at all, while it is in fact the perfect correlation in the\n",
    "first half that perfectly cancels the anti-correlation in the second half.\n",
    "This shows that it can help to not blindly rely on simplified statistics,\n",
    "but that an actual look at the data can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of features\n",
    "\n",
    "It can be useful to see which features are correlated and which are not, especially later on when we investigate what features our model uses to make decisions, to see if similar information is stored across multiple features. There are two types of correlation that are useful here: Pearson correlation and Spearman correlation.\n",
    "\n",
    "Pearson is a measure for whether two variables are linearly correlated, Spearman is a measure for whether if one variable goes up, if the other goes up as well. If you simply want a sense of whether two variables encode similar information or not, Spearman is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot two features that we know will be correlated, the `Pixels_Areas` and `SigmoidOfAreas`, the latter being the sigmoid function of the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='Pixels_Areas',y='LogOfAreas', color=\"darkblue\", xlim=[0,500]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import plot_correlation\n",
    "\n",
    "plot_correlation(df, method=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "* Which features (anti-)correlate? Are there redundant features?\n",
    "* Can you explain some of these correlations?\n",
    "* Choose a pair of correlated features and plot them. Do you notice a difference between Pearson and Spearman correlation?\n",
    "* Which features correlate with the K_Scratch defect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding\n",
    "\n",
    "This dataset uses so-called one-hot encoding, meaning that each target class has a separate column. If the specific sample is of that class, the corresponding column will have a 1, and all other columns will be 0. The other possible encoding is to have one single column to determine the class of defect, represented by a number 0 to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Pastry', 'Z_Scratch', 'K_Scratch', 'Stains',\n",
    "       'Dirtiness', 'Bumps', 'Other_Faults']].iloc[[5, 300, 500, 750, 850, 1200, 1800]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's useful to have a combined Defect_Type feature\n",
    "target = ['Pastry', 'Z_Scratch', 'K_Scratch', 'Stains',\n",
    "       'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "\n",
    "df['Defect_Type'] = 0 # Create a new feature, fill it with 0\n",
    "for t in target:\n",
    "    df.loc[df[t] == 1, 'Defect_Type'] = target.index(t) # Change one-hot encoding to a combined class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of these classes. First we'll take a count of how many of each class are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Defect_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting purposes, it's useful to sort them not by value but by index, so that we can label them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Defect_Type'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's store this in a varaible `cts` and plot it in a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = df['Defect_Type'].value_counts().sort_index()\n",
    "\n",
    "cts.plot.pie(labels=target, autopct='%1.0f%%', figsize=(8,8), fontsize=16, ylabel=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "* Which defect is most common?\n",
    "* Which is least common?\n",
    "* What do you think is the impact of the class distribution on how well a model will learn to recognize certain classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting parts of the dataframe\n",
    "\n",
    "It's also possible to give logical statements as a selector to grab specific parts of the dataframe.\n",
    "\n",
    "As an example, we can select all rows for which the type of steel is A300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This logical statement outputs a list of True and False values. \n",
    "\n",
    "df[\"TypeOfSteel_A300\"] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting this in the dataframe itself yields a selection.\n",
    "\n",
    "df[ df[\"TypeOfSteel_A300\"] == 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine the logical row selection with a selection of feature columns, for example, to get an overview of the position of defects on A300 steel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"TypeOfSteel_A300\"] == 1][[\"X_Center\", \"Y_Center\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "* What can you say about the types of steel in the dataset?\n",
    "* Take a look at the properties of A400 steel as well.\n",
    "* Plot a piechart of the counts of defect types in A300 and A400 steel. How do they differ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://www.zamet.it/en/Austenitic-Steel-A300/p/52\n",
    "\n",
    "https://www.zamet.it/en/A400-series-Ferritic-Stainless-Steels/p/54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 (optional)\n",
    "\n",
    "* Do all defects have the same size and shape? Plot histograms of the `X_Size` of a few different defects.\n",
    "* Which are typically the largest?\n",
    "* Which are typically the smallest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
